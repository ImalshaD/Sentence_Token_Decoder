{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num_output(text):\n",
    "    match = re.search(r'(?<=The answer is:\\s).*$', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"data_cache\"\n",
    "model_dir = \"model_cache\"\n",
    "ds = load_dataset(\"meta-math/MetaMathQA\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds['train']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Numerical_output']= df['response'].apply(extract_num_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerOutputs:\n",
    "    def __init__(self, input_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsOutputs:\n",
    "    def __init__(self, input_embeds, attention_mask):\n",
    "        self.input_embeds = input_embeds\n",
    "        self.attention_mask = attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentModel(ABC, nn.Module):\n",
    "    input_shape : tuple\n",
    "    output_shape : tuple\n",
    "\n",
    "    def __init__(self,input_shape, output_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        sample_input  = np.zeros(self.input_shape)\n",
    "        output = self.forward(sample_input)\n",
    "        assert output.shape == self.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NITConfig(PretrainedConfig):\n",
    "    modelName = \"NITModel\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 llm_name=\"Qwen/Qwen-1.5B\",\n",
    "                 llm_dim = 1536,\n",
    "                 freez_encoder= True,\n",
    "                 freez_llm = True,\n",
    "                 alignment_class= None,\n",
    "                 max_tokens = 200,\n",
    "                 model_cache_dir = \"model_cache\",\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm = llm_name\n",
    "        self.freez_encoder = freez_encoder\n",
    "        self.freez_llm = freez_llm\n",
    "        self.alignment_class = alignment_class\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model_cache =model_cache_dir\n",
    "        self.llm_dim = llm_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NITModel(PreTrainedModel):\n",
    "    tokenizer : AutoTokenizer\n",
    "    llm : AutoModelForCausalLM\n",
    "    alingment_model : AlignmentModel\n",
    "    embeddings: nn.Embedding\n",
    "\n",
    "    def __init__(self, config : NITConfig):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm,cache_dir=config.model_cache, padding_side='left')\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(config.llm,cache_dir=config.model_cache)\n",
    "        self.input_shape = (config.max_tokens,config.llm_dim)\n",
    "        self.output_shape = (config.max_tokens,config.llm_dim)\n",
    "        self.llm_dim = config.llm_dim\n",
    "        self.maxTokens = config.max_tokens\n",
    "        self.alingment_model = config.alignment_class(self.input_shape,self.output_shape)\n",
    "        self.device_me = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.llm.to(self.device_me)\n",
    "        # self.alingment_model.to(self.device_me)\n",
    "        self.embeddings = self.llm.get_input_embeddings()\n",
    "\n",
    "        if config.freez_llm:\n",
    "            self.freeze_lm()\n",
    "    \n",
    "    def freeze_lm(self):\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_lm(self):\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def get_inputIds(self, texts):\n",
    "        encoding = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.maxTokens\n",
    "        )\n",
    "        output = TokenizerOutputs(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self, inputs : TokenizerOutputs):\n",
    "        return EmbeddingsOutputs(self.embeddings(inputs.input_ids), inputs.attention_mask)\n",
    "    \n",
    "    def get_embeddings_altered(self, inputs : TokenizerOutputs):\n",
    "        return EmbeddingsOutputs(self.alingment_model(self.get_embeddings(inputs).input_embeds), inputs.attention_mask)\n",
    "    \n",
    "    def original_pipeline(self, texts):\n",
    "        return self.get_embeddings(self.get_inputIds(texts))\n",
    "    \n",
    "    def altered_pipeline(self, texts):\n",
    "        return self.get_embeddings_altered(self.get_inputIds(texts))\n",
    "\n",
    "    def get_outputs(self, embeddings : EmbeddingsOutputs):\n",
    "        return self.llm(\n",
    "            attention_mask=embeddings.attention_mask,\n",
    "            inputs_embeds=embeddings.input_embeds,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    def pipeline_outputs(self, texts, altered = False, grad = False):\n",
    "        if altered:\n",
    "            return self.get_outputs(self.altered_pipeline(texts))\n",
    "        return self.get_outputs(self.original_pipeline(texts))\n",
    "    \n",
    "    def forward(self , texts):\n",
    "        \n",
    "        # TODO: Modify accordingly\n",
    "        batch_size = len(texts)\n",
    "\n",
    "        original_output = self.pipeline_outputs(texts)\n",
    "        altered_outputs = self.pipeline_outputs(texts, altered=True)\n",
    "\n",
    "        original_logits = original_output.logits\n",
    "        altered_logits = altered_outputs.logits\n",
    "\n",
    "        original_prob = F.softmax(original_logits, dim=1)\n",
    "        original_labels = torch.argmax(original_prob, dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(altered_logits,original_labels)\n",
    "        \n",
    "        # loss = rearrange(loss, '(b s) -> b s', b=batch_size)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=altered_logits,\n",
    "            hidden_states=altered_outputs.hidden_states,\n",
    "            attentions=altered_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_tokens = 100\n",
    "embedding_lenght = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=1\n",
    "split_size = 395000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_query = df[\"query\"][split_size*(split-1):split_size*(split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = X_train_query.to_numpy()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test = train_test_split(data_array, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment Model Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAlignmentModel(AlignmentModel):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        # Calculate flattened dimensions for linear transformation\n",
    "        print(\"Testing Purpose only Don't use this model.\")\n",
    "        input_dim = input_shape[0] * input_shape[1]\n",
    "        output_dim = output_shape[0] * output_shape[1]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(input_dim, 1000)\n",
    "        self.linear2 = nn.Linear(1000, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input to match linear layer's expected shape\n",
    "        x = self.flatten(x)\n",
    "        # Pass through linear layer\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Reshape output to match output_shape\n",
    "        x = x.view(-1,self.output_shape[0],self.output_shape[1])  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAlignementModel(AlignmentModel):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        input_dim = input_shape[0] * input_shape[1]\n",
    "        output_dim = output_shape[0] * output_shape[1]\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024*2),  # Input size: 200*1536, Output size: 1024\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024*2,1024),\n",
    "            # nn.LeakyReLU(0.001),\n",
    "            # nn.Linear(1024*2, 1024)             # Bottleneck size: 64\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(1024, 1024*2),             # Input size: 64, Output size: 256\n",
    "            # nn.LeakyReLU(0.001),\n",
    "            nn.Linear(1024,1024*2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024*2, output_dim),    # Output size: 200*1536\n",
    "            nn.Tanh()                  #\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input tensor\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.view(-1,self.output_shape[0],self.output_shape[1])   # Reshape to original image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions of save/load Model/Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename='checkpoint.pth'):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename='checkpoint.pth'):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded from {filename}, starting at epoch {start_epoch} with loss {loss}\")\n",
    "        return start_epoch, loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(model, filename='model.pth'):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the model\n",
    "def load_model(model, filename='model.pth'):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Model loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NITConfig(\n",
    "    llm_name=\"Qwen/Qwen2.5-Math-1.5B\",\n",
    "    llm_dim=embedding_lenght,\n",
    "    freeze_encoder=True,\n",
    "    alignment_class=LinearAlignementModel,\n",
    "    max_tokens=max_tokens,\n",
    "    model_cache_dir= model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit_model = NITModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with GPU support\n",
    "def train_model(model: NITModel, train_loader, val_loader, optimizer,start_epoch=0 ,num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(start_epoch,num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for inputs in tqdm(train_loader, desc = f'epoch_{epoch+1}/{num_epochs}: loss {train_loss}'):\n",
    "            \n",
    "            batch_size = len(inputs)\n",
    "\n",
    "            loss = model.forward(inputs).loss\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_size  # Accumulate training loss\n",
    "            \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in val_loader:\n",
    "                batch_size = len(inputs)\n",
    "                loss = model.forward(inputs).loss\n",
    "                val_loss += loss.item() * batch_size  # Accumulate validation loss\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        try:\n",
    "            save_checkpoint(nit_model,optimizer,epoch,train_loss)\n",
    "        except:\n",
    "            print('Checkpoint saving Failed.')\n",
    "\n",
    "        try:\n",
    "            save_model(model)\n",
    "        except:\n",
    "            print(\"Saving Model failed\")\n",
    "        # Print losses\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, Val Loss: {val_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(nit_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "start_epoch,_  = load_checkpoint(nit_model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(nit_model, train_loader, test_loader, optimizer,start_epoch=start_epoch ,num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig With Pytorch-lightining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NITModelLightning(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super(NITModelLightning, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.model(batch).loss\n",
    "        self.log(\"train_loss\", loss.mean(), on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.model(batch).loss\n",
    "        self.log(\"val_loss\", loss.mean(), on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    strategy=\"ddp_spawn\",\n",
    "    max_epochs=1,\n",
    "    accelerator='gpu',  # This will automatically choose 'gpu' if available, else 'cpu'\n",
    "    devices= 4 if torch.cuda.is_available() else None,  # Use 1 GPU if available\n",
    "    log_every_n_steps=100  # Log progress every 10 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NITModel(\n",
       "  (llm): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 1536)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "  )\n",
       "  (alingment_model): LinearAlignementModel(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=153600, out_features=2048, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=2048, out_features=153600, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (embeddings): Embedding(151936, 1536)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nit_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit_model_lightining = NITModelLightning(nit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 08:08:21.962000 3986 torch/multiprocessing/spawn.py:160] Terminating process 4192 via signal SIGTERM\n",
      "W1028 08:08:21.967000 3986 torch/multiprocessing/spawn.py:160] Terminating process 4251 via signal SIGTERM\n",
      "W1028 08:08:21.969000 3986 torch/multiprocessing/spawn.py:160] Terminating process 4310 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 937, in _run\n    self.strategy.setup_environment()\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 153, in setup_environment\n    super().setup_environment()\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 128, in setup_environment\n    self.accelerator.setup_device(self.root_device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py\", line 46, in setup_device\n    _check_cuda_matmul_precision(device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 161, in _check_cuda_matmul_precision\n    if not torch.cuda.is_available() or not _is_ampere_or_later(device):\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 155, in _is_ampere_or_later\n    major, _ = torch.cuda.get_device_capability(device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 509, in get_device_capability\n    prop = get_device_properties(device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 523, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnit_model_lightining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:203\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    201\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    202\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 937, in _run\n    self.strategy.setup_environment()\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 153, in setup_environment\n    super().setup_environment()\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 128, in setup_environment\n    self.accelerator.setup_device(self.root_device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py\", line 46, in setup_device\n    _check_cuda_matmul_precision(device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 161, in _check_cuda_matmul_precision\n    if not torch.cuda.is_available() or not _is_ampere_or_later(device):\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py\", line 155, in _is_ampere_or_later\n    major, _ = torch.cuda.get_device_capability(device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 509, in get_device_capability\n    prop = get_device_properties(device)\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 523, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"/home/imalsha.20/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(nit_model_lightining, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = [X_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How many ways are there to put 4 distinguishable balls into 2 distinguishable boxes?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "# original_outputs = nit_model.pipeline_outputs(q)\n",
    "# altered_outputs = nit_model.pipeline_outputs(q,altered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0090, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original_outputs.logits.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.0091, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# altered_outputs.logits.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoded_original \u001b[38;5;241m=\u001b[39m \u001b[43mnit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3959\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3944\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3961\u001b[0m             seq,\n\u001b[1;32m   3962\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3963\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3964\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3967\u001b[0m     ]\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3960\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3944\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3960\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3967\u001b[0m     ]\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3999\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3996\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3997\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4003\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4004\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fyp/Sentence_Token_Decoder/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    653\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 654\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    660\u001b[0m )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# decoded_original = nit_model.tokenizer.batch_decode(original_outputs.logits, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original  = nit_model.altered_pipeline(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs= nit_model.llm.generate(\n",
    "#             input_ids=None,\n",
    "#             inputs_embeds=original.input_embeds,\n",
    "#             attention_mask=original.attention_mask,\n",
    "#             pad_token_id=nit_model.tokenizer.pad_token_id,  # Padding token ID\n",
    "#             eos_token_id=nit_model.tokenizer.eos_token_id,  # End-of-sequence token ID\n",
    "#             no_repeat_ngram_size=2,\n",
    "#             return_dict=True\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' M M MM MM MMM MMM MMMMMMMMMMMMMNNNNNNNNOOOOORRRRRRRRRNNOOONNNOOOOORRNNRRNNNRNNRNRNRRNNOORNNRNRRNRNRNOONNOOPPPPPNNPPPNPPNPPPNNPNNNPPOOPPOOOPNNPOONNPOPPPOPOOOOPPNPOORPPRPPRRPPPPPNNPPPPOPNNOPONNOPNPNOPPNOPPNOPOPPOPOPNOIPNOPNOIPOIPNNINNIPPOINNOININIPPIIPPPPIPOPIPIPPINPOIINPPIPPNPIOPPIPNPNIPNOPIOINNIPINPNIOPINOPIPIPNIOPNIPOOIPOIIPOIOIIPOPIPOPPIIPIPOPNNPIONPNINNNINPIINPNIPINNNIIPNPNNNPPINPIOOIIOOIPOOIPOIIPIIIPIPPPIIIPIOIOPIIOPOIPOPIPPOPPOPNINNIIINIIIIIOOOIIOOINOOOIINIOOIIOIIOIIIIONNIIIONNOIIONIIINOIIIIIIOINIIIINIINOINONOOONINNONNOOIINOIPIONOPOIONPOINOPOPINPOIPPPOPPIPIONPNIINNONNONONOPOPONPONOIOIPONPONPPONPOPONPIIONPINOOPIONOPPINPNNINNOPNPOPIONPOPINPOPOPPOPPNPOPNNINOONIPNOPPONOPPINOPPNPNONNIOONIPNPINPIINOPIIOPIIPPNNNOPNNPOPNONPNONNOPNOIPPNOIONNNIOOPNOPINPINNIONPINOPIPNNIONIPNONPONONOPNONNNNONNOPONPINOPPIPPOPIPPIPIPIONINPPIPNOPINNOPNPIPNIINOIONIONINIONIOIOPOIORPOIRPOROIPRIPORIPROPORORORPOROPOROPRPORPORPNORNORINORIONORIOORIORORPINORPIORPORORPOPORPOROPIROPIRPIROPPIRPPIRRORRNORNOIRNRNOIORIORRPNRINRIRORIRIRRROORROIRROIORIRIORROINIRNOINOORIORIIOROOIORIIRIOIRRIORRIIRRRROOIRNNORONORNRORORNORINOIRIONIRRNIRINRRINIORINRNINNRINRIINROIONRIOIORIONIORIOIONRNIOINOIORNIRIORSORSNOROSORSOORSORORORSORSOSORSRSORSRORSSORRSORSROSRORSRRSORSIRSRISORISORSISRSIRISROISIRRSISRRISRISSRSSSRSRNRSRSRNSRSRRSRNSRRNSRSRNRNSRNRSRNNSNSNNSNNNSNRSSNSSRNNSSNNSRSNNSRSSSRNSSNSSSNSSSSRSSNNRSSNSNSSNNNSSNNRISNSIRNSISNNISNISRNISINISINSISISISSISSSISIIISRSSNRIISNRIRSNISNIISRIIISINRNIIRNRRIIRRNNIRIRRNSISSISSNNISSNSINNSINSNNRSNNRIIRSNSNIIRSNNIRSRSNIRSRRIRISSRRSSRRRSSRRRSRSSRSSRSNSSNSSSNNSNRSSSSRNSSRSNRSRIRSSRSSIRRSSRISRIRSIIRSSIIRIIIRIIIIRIRSIRISRISIIIISSNIRSNSIISSIIIRSISSIISIRRISSSIISNSSSSISSSSSIINSSSNIIISSIIIRRSSINSRINRSINRSSIRNSSIRNISSNRSSIINISSIRINSIRIVIRVIRVIIRVVIRVRVIVIVVVRVVVRRVRSIVRIVRRIVRSVSSIVSSVSRVSSIIVSRIVSIVSVSSVVSSSVVSVIVVVIVVSIVVRIVRVIVRIIVIIIVISSIIIIVISIVSISSVIIVISSIVSSISSSSIIRRIVVISSIIIVIIVVIIVIISVSIIVIIIIRSIVRSSIVIRSRISSRIISSRRIIIRSSRIRSSSRRSVRSRISSIIRSIIRRSIIIRSIIISRIVISRSSRVSSVRSSVSIRSVSRVVSRVRIRVSRSVIIRSIRRVSRIVRIVSRRVVRSVVRRVSIIVSISVISSIISRVRISRVIRRVRSRVRRVRVRVIIRRVRVSIRRIRVISIRVASIRIASIRASIRRAIRAVRVSRAIVRAIIVRNIVNIVNSIVNNIVNRIVINIVNIIVSNIVNVNVSNSVNIVVNNSVNSNVNSVIINVSNVNNVNRVVSNNVVNSRVSINVSNVRNVIINVNSVRNSRVNINVNNVVNSSIVNSSVNVSSVNVSINVSSNVIVINSIVINVIVNsIVIRRNVNRNVIRNVRSVNIRVNRSINVIRINVNRINVNNINVRRNVNNNVRRRVNRVVNNRVNNVRNNVNRRVNVRNRVRNVNVRNNNVIINSVINSNIIIINVIIINVVINVVIIIINSIIVNIINIIIIIRRINVRSIRRINVSRINVRIINVVRINVRNINVNVINVINVININVINSNSINVSSIINVSNRSINSRSIRRINVRINVIISRNSIINVSVNSVVNSVSINSINSSVNINSINSININSINVVSIIISSINSRRSNRNVSRNVSNRRNSSRSNVSSIINSRSSRNRIINSRNIIRNIRSINVNSSRIINSRIISRNNSVNNVSNIINVIRSINSRINSRVNSIRRINSSRINSRIRRINSIRSINSSIIPNSPNNSPIIVPIIRPIVPIVPPIPVPIVVPIVRPIVIPIVSPIVNPIISPISSPIISSPIPSPIPRPIRSPIRVPIIPSPIIRSPIRSSPISRPISPIPISPPIISPPISVPISSIPIINSPINSPVNSPSIVPSVPSPVPSIPPSISPSPSIIPSISIPSSIPSPSSPSPPSSPPSPSPSPISSPVSPSVPSVIIPVSPVIPSIVSPVIIPSIPSVIPVIISPSPIPSPIIPISPSSPSPPSIIIPSISPISPPSPIIVIPIVPISPVIPISIPPSPISSPIVISPISPVIIPPISPNIVPNVNPVPNPVPNVPVPPVPIPVPVPVIPIPVIPPVIVPIPVPVVPPVPSPVPSSPNPSNPSNSNPSSNPSPNISPNSIPSNIPSIIPSSIIPSNSIPSIPSINPSINSPSNPSNPNSPPSNPPSPPPSPNSSSPIPSISSIPISSIPSIPPSSIPPSSIIPPISPSSIISPSPIISSPPSIPISSIPSSIPSIIISPIPIPSISSIPPIPPIPISSIPPVPPSVPPPVSSSPSISIPSPPISPIPSPISPVPISVPISPVSIPVSIPPVIISSPVSSIPIPSSISPIPSPIPSSIISSIPVIPSIRIPIRPSIRPVIRPVIPRVPRPVPRPPVVPSIPPVSPPVSIPSVIIMSIVPPIVPVIVVPIVIPPVPVSVPSSIVPSPIVPIPPVIPPIPSVPIPSVVPVIPVVIVSPIPVSISPVIPVVIPVIPVIVVIPVPPIPISPPIPIPPIVIPSINVPIVIPPIVISIPVISIVSPIVIPIVVIPIPPVPINPVV IPSIVPIPIPINVIPVNIPNVIPVINIVNPINVPNVPNNPVNIPSNNPSNNPINIVPINIPPINPVNPVPNSPINPSNVPINVNPISNPIPINSIPSINIPSINSIPPNSIPPINSPPNSPNSPIPNSSPIPNISPINNSISPNNIPSNPIPINSPNIPSNPINSNPISPINSPIPINSPNIRPNPRPNRPINPRNPRIPRNIPRPIPRIIPRRPNRRIPRSIPSRPNRSRPISRPIRPRIRRPIRRIPPRPRRPRPIRPPIRPIPIRPINIRNPIRSPIIRSPIRISPIRIPSIRRPIIRRSPSRIPIRSIPRSSRPSSRPSPRSPRSPPSRPRSRPSRPSRPSPIRRSPRPVPRSIRPRSIPRVIPVRIPPRSIVPRIVRPVRPRPSPRSPPRSPIPRISPRISPVRSPVRPSRRPRSVPRSSPRSRSPRRSIPPSPIPVSRSPRRSPIRISPISRSPRSSPRRSSSPIRSIPSRRPSRSISSPRSIIRSIPSRIPIRIIPSRSIRSIRSIPPIRVPIRVIPIRIPPVRVPVRPPVRPVRRPVRSVPRRPVRPVSRPVIROPVIRSVPIRRPRVPRIVPPRVRPNVRNPVRVNPRNRPNRVPRNPNRPRNNPRNSPRSNPRNVPRINRPNNRPNRPNRPIRNPRRNPNRNPINRIPNRNPNNPNIPIPVRPINPRPINPPNPNPVIPNNPIVPPPPINPPIPPIPIV PINPRRPIVPRSPPPRRRPIPINRNPIRRPNRIPRRIPIPIRRPPRPPIPWRPNIRSPPRSNPRIPPRNPPRIIPPINNPINVPRINVPPINVPIPINVPNNRPPNRPVNRVPNRVNPNNVPPNVPNVVNPNVPSNRPSRNPSRVPPRVPSVPNPVSNPVVPNVSNVSPNPVNPPVNVPNVVPVNPSVNNPPN']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nit_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0022, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.mse_loss(nit_model.altered_pipeline(q).input_embeds, nit_model.original_pipeline(q).input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15566/3192020436.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoint.pth, starting at epoch 0 with loss -7550580.039974684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, -7550580.039974684)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_checkpoint(nit_model,optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
